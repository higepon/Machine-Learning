{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Using convolutional net on MNIST dataset of handwritten digit\n",
    "(http://yann.lecun.com/exdb/mnist/)\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "N_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Read in data\n",
    "# using TF Learn's built in function to load MNIST data to the folder data/mnist\n",
    "mnist = input_data.read_data_sets(\"./data/mnist\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Step 2: Define paramaters for the model\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 128\n",
    "SKIP_STEP = 10\n",
    "DROPOUT = 0.75\n",
    "N_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# Step 3: create placeholders for features and labels\n",
    "# each image in the MNIST data is of shape 28*28 = 784\n",
    "# therefore, each image is represented with a 1x784 tensor\n",
    "# We'll be doing dropout for hidden layer so we'll need a placeholder\n",
    "# for the dropout probability too\n",
    "# Use None for shape so we can change the batch_size once we've built the graph\n",
    "with tf.name_scope('data'):\n",
    "    X = tf.placeholder(tf.float32, [None, 784], name=\"X_placeholder\")\n",
    "    Y = tf.placeholder(tf.float32, [None, 10], name=\"Y_placeholder\")\n",
    "\n",
    "dropout = tf.placeholder(tf.float32, name='dropout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Step 4 + 5: create weights + do inference\n",
    "# the model is conv -> relu -> pool -> conv -> relu -> pool -> fully connected -> softmax\n",
    "\n",
    "global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "with tf.variable_scope('conv1') as scope:\n",
    "    # first, reshape the image to [BATCH_SIZE, 28, 28, 1] to make it work with tf.nn.conv2d\n",
    "    # use the dynamic dimension -1\n",
    "    X_reshaped = tf.reshape(X, [-1, 28, 28, 1], name=\"X_reshaped\")\n",
    "\n",
    "    # create kernel variable of dimension [5, 5, 1, 32]\n",
    "    # use tf.truncated_normal_initializer()\n",
    "    W = tf.get_variable('weights', [5, 5, 1, 32], initializer=tf.truncated_normal_initializer())\n",
    "\n",
    "    # create biases variable of dimension [32]\n",
    "    # use tf.random_normal_initializer()\n",
    "    b = tf.get_variable('biases', [32], initializer=tf.random_normal_initializer())\n",
    "    \n",
    "    # apply tf.nn.conv2d. strides [1, 1, 1, 1], padding is 'SAME'\n",
    "    conv = tf.nn.conv2d(X_reshaped, W, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "    \n",
    "    # apply relu on the sum of convolution output and biases\n",
    "    conv2 = tf.nn.relu(conv + b, name='conv1')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('pool1') as scope:\n",
    "    # apply max pool with ksize [1, 2, 2, 1], and strides [1, 2, 2, 1], padding 'SAME'\n",
    "    pooled = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pooled')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('conv2') as scope:\n",
    "    # similar to conv1, except kernel now is of the size 5 x 5 x 32 x 64\n",
    "    kernel = tf.get_variable('kernels', [5, 5, 32, 64], \n",
    "                        initializer=tf.truncated_normal_initializer())\n",
    "    biases = tf.get_variable('biases', [64],\n",
    "                        initializer=tf.random_normal_initializer())\n",
    "    conv = tf.nn.conv2d(pooled, kernel, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    conv2 = tf.nn.relu(conv + biases, name=scope.name)\n",
    "\n",
    "    # output is of dimension BATCH_SIZE x 14 x 14 x 64\n",
    "\n",
    "with tf.variable_scope('pool2') as scope:\n",
    "    # similar to pool1\n",
    "    pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                            padding='SAME')\n",
    "\n",
    "    # output is of dimension BATCH_SIZE x 7 x 7 x 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('fc') as scope:\n",
    "    # use weight of dimension 7 * 7 * 64 x 1024\n",
    "    input_features = 7 * 7 * 64\n",
    "    \n",
    "    # create weights and biases\n",
    "    W_fc = tf.get_variable('weights_fc', [input_features, 1024])\n",
    "    b_fc = tf.get_variable('biases_fc', [BATCH_SIZE, 1024])\n",
    "\n",
    "    # reshape pool2 to 2 dimensional\n",
    "    pool2 = tf.reshape(pool2, [-1, input_features])\n",
    "\n",
    "    # apply relu on matmul of pool2 and w + b\n",
    "    fc = tf.nn.relu(tf.matmul(pool2, W_fc) + b_fc)\n",
    "    \n",
    "    # apply dropout\n",
    "    fc = tf.nn.dropout(fc, dropout, name='relu_dropout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('softmax_linear') as scope:\n",
    "    # this you should know. get logits without softmax\n",
    "    # you need to create weights and biases\n",
    "    W_s = tf.get_variable('W_s', [1024, 10])\n",
    "    b_s = tf.get_variable('b_s', [1, 10])\n",
    "    out = tf.matmul(fc, W_s)+ b_s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 6: define loss function\n",
    "# use softmax cross entropy with logits as the loss function\n",
    "# compute mean cross entropy, softmax is applied internally\n",
    "with tf.name_scope('loss'):\n",
    "    # you should know how to do this too\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(out, Y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 7: define training op\n",
    "# using gradient descent with learning rate of LEARNING_RATE to minimize cost\n",
    "# don't forgot to pass in global_step\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss, global_step=global_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.1864\n",
      "134.14\n",
      "185.316\n",
      "153.681\n",
      "156.261\n",
      "60.9879\n",
      "27.6666\n",
      "15.5446\n",
      "6.64192\n",
      "2.53175\n",
      "Average loss at step 10:  78.6\n",
      "1.85028\n",
      "1.93998\n",
      "1.90275\n",
      "1.63934\n",
      "1.39433\n",
      "1.61144\n",
      "1.37294\n",
      "1.88616\n",
      "1.33188\n",
      "1.52716\n",
      "Average loss at step 20:   1.6\n",
      "1.35512\n",
      "1.30433\n",
      "1.28596\n",
      "1.24391\n",
      "1.14077\n",
      "1.27149\n",
      "1.38269\n",
      "1.09726\n",
      "1.0485\n",
      "1.15405\n",
      "Average loss at step 30:   1.2\n",
      "1.15828\n",
      "0.921469\n",
      "0.882207\n",
      "1.01269\n",
      "1.2039\n",
      "1.24498\n",
      "0.976512\n",
      "0.889433\n",
      "1.04719\n",
      "0.870906\n",
      "Average loss at step 40:   1.0\n",
      "0.780757\n",
      "0.77606\n",
      "1.09103\n",
      "1.14549\n",
      "1.09111\n",
      "1.00879\n",
      "0.839388\n",
      "0.772447\n",
      "0.772932\n",
      "1.06684\n",
      "Average loss at step 50:   0.9\n",
      "0.896091\n",
      "0.704871\n",
      "0.517724\n",
      "0.51573\n",
      "0.738422\n",
      "0.672576\n",
      "0.738668\n",
      "0.73938\n",
      "0.511591\n",
      "0.621581\n",
      "Average loss at step 60:   0.7\n",
      "0.77831\n",
      "0.695029\n",
      "0.547368\n",
      "0.744435\n",
      "0.363315\n",
      "0.502201\n",
      "0.612616\n",
      "0.93011\n",
      "0.888483\n",
      "0.587373\n",
      "Average loss at step 70:   0.7\n",
      "0.519983\n",
      "0.514459\n",
      "0.599801\n",
      "0.62059\n",
      "0.797866\n",
      "0.594768\n",
      "0.463431\n",
      "0.667391\n",
      "0.582274\n",
      "0.51969\n",
      "Average loss at step 80:   0.6\n",
      "0.609779\n",
      "0.481946\n",
      "0.466194\n",
      "1.01578\n",
      "0.732936\n",
      "0.473403\n",
      "0.739276\n",
      "0.716617\n",
      "0.655971\n",
      "0.658179\n",
      "Average loss at step 90:   0.7\n",
      "0.415934\n",
      "0.452533\n",
      "0.367159\n",
      "0.390572\n",
      "0.491953\n",
      "0.464235\n",
      "0.844181\n",
      "0.584879\n",
      "0.737587\n",
      "0.527014\n",
      "Average loss at step 100:   0.5\n",
      "0.561283\n",
      "0.460266\n",
      "0.686069\n",
      "0.77594\n",
      "0.685913\n",
      "0.438782\n",
      "0.424187\n",
      "0.480231\n",
      "0.536746\n",
      "0.369191\n",
      "Average loss at step 110:   0.5\n",
      "0.562834\n",
      "0.357784\n",
      "0.594853\n",
      "0.570914\n",
      "0.365663\n",
      "0.36619\n",
      "0.44703\n",
      "0.743927\n",
      "0.708523\n",
      "0.654338\n",
      "Average loss at step 120:   0.5\n",
      "0.690526\n",
      "0.505536\n",
      "0.67543\n",
      "0.591935\n",
      "0.463435\n",
      "0.560563\n",
      "0.471351\n",
      "0.42003\n",
      "0.456577\n",
      "0.256152\n",
      "Average loss at step 130:   0.5\n",
      "0.402328\n",
      "0.405991\n",
      "0.565273\n",
      "0.368507\n",
      "0.723624\n",
      "0.400062\n",
      "0.495359\n",
      "0.584999\n",
      "0.684835\n",
      "0.484587\n",
      "Average loss at step 140:   0.5\n",
      "0.547823\n",
      "0.419056\n",
      "0.384414\n",
      "0.506697\n",
      "0.697803\n",
      "0.47505\n",
      "0.549596\n",
      "0.475174\n",
      "0.461564\n",
      "0.42878\n",
      "Average loss at step 150:   0.5\n",
      "0.544188\n",
      "0.400858\n",
      "0.684607\n",
      "0.56629\n",
      "0.555216\n",
      "0.592714\n",
      "0.570956\n",
      "0.438459\n",
      "0.820175\n",
      "0.541769\n",
      "Average loss at step 160:   0.6\n",
      "0.418586\n",
      "0.466324\n",
      "0.501475\n",
      "0.6005\n",
      "0.647636\n",
      "0.275524\n",
      "0.400666\n",
      "0.492426\n",
      "0.332423\n",
      "0.322068\n",
      "Average loss at step 170:   0.4\n",
      "0.335133\n",
      "0.381603\n",
      "0.345885\n",
      "0.321908\n",
      "0.390155\n",
      "0.302993\n",
      "0.416782\n",
      "0.298489\n",
      "0.396061\n",
      "0.603229\n",
      "Average loss at step 180:   0.4\n",
      "0.536981\n",
      "0.641589\n",
      "0.69223\n",
      "0.57631\n",
      "0.300123\n",
      "0.45476\n",
      "0.336754\n",
      "0.319623\n",
      "0.395176\n",
      "0.512663\n",
      "Average loss at step 190:   0.5\n",
      "0.339918\n",
      "0.465557\n",
      "0.369399\n",
      "0.364548\n",
      "0.237443\n",
      "0.493783\n",
      "0.314993\n",
      "0.276055\n",
      "0.413229\n",
      "0.315769\n",
      "Average loss at step 200:   0.4\n",
      "0.485847\n",
      "0.358309\n",
      "0.483318\n",
      "0.678931\n",
      "0.403609\n",
      "0.557132\n",
      "0.422111\n",
      "0.454866\n",
      "0.300424\n",
      "0.409334\n",
      "Average loss at step 210:   0.5\n",
      "0.476265\n",
      "0.349017\n",
      "0.486\n",
      "0.485775\n",
      "0.253921\n",
      "0.330831\n",
      "0.260413\n",
      "0.396769\n",
      "0.621895\n",
      "0.38255\n",
      "Average loss at step 220:   0.4\n",
      "0.553815\n",
      "0.588818\n",
      "0.45311\n",
      "0.417672\n",
      "0.344982\n",
      "0.422802\n",
      "0.312617\n",
      "0.337646\n",
      "0.400665\n",
      "0.243566\n",
      "Average loss at step 230:   0.4\n",
      "0.310563\n",
      "0.418233\n",
      "0.400675\n",
      "0.61295\n",
      "0.441796\n",
      "0.458232\n",
      "0.295235\n",
      "0.366747\n",
      "0.316798\n",
      "0.501907\n",
      "Average loss at step 240:   0.4\n",
      "0.294519\n",
      "0.311804\n",
      "0.592322\n",
      "0.295081\n",
      "0.45588\n",
      "0.478044\n",
      "0.671243\n",
      "0.355234\n",
      "0.345743\n",
      "0.182221\n",
      "Average loss at step 250:   0.4\n",
      "0.293141\n",
      "0.248005\n",
      "0.406267\n",
      "0.490002\n",
      "0.339752\n",
      "0.493849\n",
      "0.392699\n",
      "0.425136\n",
      "0.303023\n",
      "0.222351\n",
      "Average loss at step 260:   0.4\n",
      "0.297008\n",
      "0.524647\n",
      "0.565192\n",
      "0.472758\n",
      "0.210552\n",
      "0.317385\n",
      "0.437228\n",
      "0.434923\n",
      "0.483936\n",
      "0.480826\n",
      "Average loss at step 270:   0.4\n",
      "0.349268\n",
      "0.372376\n",
      "0.532826\n",
      "0.594377\n",
      "0.192833\n",
      "0.31171\n",
      "0.296766\n",
      "0.307112\n",
      "0.241955\n",
      "0.409471\n",
      "Average loss at step 280:   0.4\n",
      "0.712973\n",
      "0.402476\n",
      "0.440246\n",
      "0.280391\n",
      "0.222695\n",
      "0.373266\n",
      "0.365592\n",
      "0.179719\n",
      "0.645561\n",
      "0.218772\n",
      "Average loss at step 290:   0.4\n",
      "0.272978\n",
      "0.339028\n",
      "0.262074\n",
      "0.546953\n",
      "0.526066\n",
      "0.353375\n",
      "0.224189\n",
      "0.573816\n",
      "0.574141\n",
      "0.270922\n",
      "Average loss at step 300:   0.4\n",
      "0.516376\n",
      "0.296262\n",
      "0.36265\n",
      "0.369798\n",
      "0.732727\n",
      "0.284399\n",
      "0.319619\n",
      "0.350892\n",
      "0.279313\n",
      "0.221152\n",
      "Average loss at step 310:   0.4\n",
      "0.382966\n",
      "0.322411\n",
      "0.32415\n",
      "0.210225\n",
      "0.15828\n",
      "0.163761\n",
      "0.207076\n",
      "0.472007\n",
      "0.556798\n",
      "0.460406\n",
      "Average loss at step 320:   0.3\n",
      "0.293751\n",
      "0.131794\n",
      "0.424818\n",
      "0.435473\n",
      "0.752008\n",
      "0.304639\n",
      "0.562665\n",
      "0.183275\n",
      "0.22885\n",
      "0.506797\n",
      "Average loss at step 330:   0.4\n",
      "0.356616\n",
      "0.292973\n",
      "0.372935\n",
      "0.304489\n",
      "0.310282\n",
      "0.266149\n",
      "0.269975\n",
      "0.304932\n",
      "0.277702\n",
      "0.448343\n",
      "Average loss at step 340:   0.3\n",
      "0.421046\n",
      "0.224418\n",
      "0.315993\n",
      "0.245891\n",
      "0.257957\n",
      "0.341494\n",
      "0.352268\n",
      "0.315148\n",
      "0.123039\n",
      "0.181098\n",
      "Average loss at step 350:   0.3\n",
      "0.366827\n",
      "0.242492\n",
      "0.345217\n",
      "0.327868\n",
      "0.303285\n",
      "0.34629\n",
      "0.393691\n",
      "0.139439\n",
      "0.435664\n",
      "0.216241\n",
      "Average loss at step 360:   0.3\n",
      "0.258959\n",
      "0.453666\n",
      "0.199688\n",
      "0.264127\n",
      "0.316826\n",
      "0.0629489\n",
      "0.0789078\n",
      "0.31098\n",
      "0.145382\n",
      "0.161043\n",
      "Average loss at step 370:   0.2\n",
      "0.250429\n",
      "0.0776574\n",
      "0.0560309\n",
      "0.0697746\n",
      "0.596579\n",
      "0.344358\n",
      "0.186298\n",
      "0.424888\n",
      "0.0630481\n",
      "0.266884\n",
      "Average loss at step 380:   0.2\n",
      "0.193308\n",
      "0.333074\n",
      "0.428028\n",
      "0.187715\n",
      "0.119497\n",
      "0.186019\n",
      "0.430387\n",
      "0.291596\n",
      "0.289665\n",
      "0.350706\n",
      "Average loss at step 390:   0.3\n",
      "0.335792\n",
      "0.257587\n",
      "0.413409\n",
      "0.256202\n",
      "0.374745\n",
      "0.288581\n",
      "0.243093\n",
      "0.216392\n",
      "0.303665\n",
      "0.44335\n",
      "Average loss at step 400:   0.3\n",
      "0.253259\n",
      "0.313953\n",
      "0.394878\n",
      "0.422359\n",
      "0.456901\n",
      "0.295073\n",
      "0.358588\n",
      "0.312182\n",
      "0.325504\n",
      "0.375638\n",
      "Average loss at step 410:   0.4\n",
      "0.13491\n",
      "0.447283\n",
      "0.211819\n",
      "0.252777\n",
      "0.324801\n",
      "0.255063\n",
      "0.364188\n",
      "0.248756\n",
      "0.258965\n",
      "0.234745\n",
      "Average loss at step 420:   0.3\n",
      "0.287767\n",
      "0.208611\n",
      "0.380651\n",
      "0.373834\n",
      "0.32159\n",
      "0.344445\n",
      "0.31001\n",
      "0.267621\n",
      "0.407783\n",
      "0.236647\n",
      "Average loss at step 430:   0.3\n",
      "0.371343\n",
      "0.235604\n",
      "0.18075\n",
      "0.2745\n",
      "0.482055\n",
      "0.282957\n",
      "0.372445\n",
      "0.196715\n",
      "0.339596\n",
      "0.291007\n",
      "Average loss at step 440:   0.3\n",
      "0.275923\n",
      "0.368102\n",
      "0.229789\n",
      "0.325649\n",
      "0.273856\n",
      "0.184146\n",
      "0.39029\n",
      "0.439047\n",
      "0.314342\n",
      "0.251741\n",
      "Average loss at step 450:   0.3\n",
      "0.265237\n",
      "0.350998\n",
      "0.256091\n",
      "0.314654\n",
      "0.324615\n",
      "0.321199\n",
      "0.149851\n",
      "0.291753\n",
      "0.271171\n",
      "0.22829\n",
      "Average loss at step 460:   0.3\n",
      "0.237961\n",
      "0.309556\n",
      "0.207682\n",
      "0.37557\n",
      "0.297071\n",
      "0.212871\n",
      "0.27068\n",
      "0.240624\n",
      "0.258251\n",
      "0.302495\n",
      "Average loss at step 470:   0.3\n",
      "0.332845\n",
      "0.236597\n",
      "0.313198\n",
      "0.374836\n",
      "0.295645\n",
      "0.222077\n",
      "0.314041\n",
      "0.214825\n",
      "0.167273\n",
      "0.190384\n",
      "Average loss at step 480:   0.3\n",
      "0.294774\n",
      "0.422938\n",
      "0.170186\n",
      "0.280603\n",
      "0.381822\n",
      "0.359846\n",
      "0.394177\n",
      "0.2206\n",
      "0.26512\n",
      "0.219951\n",
      "Average loss at step 490:   0.3\n",
      "0.245076\n",
      "0.26755\n",
      "0.324879\n",
      "0.26575\n",
      "0.261015\n",
      "0.22066\n",
      "0.380747\n",
      "0.313878\n",
      "0.360775\n",
      "0.221996\n",
      "Average loss at step 500:   0.3\n",
      "0.316678\n",
      "0.224836\n",
      "0.160925\n",
      "0.20254\n",
      "0.173134\n",
      "0.210184\n",
      "0.131739\n",
      "0.433326\n",
      "0.174061\n",
      "0.139063\n",
      "Average loss at step 510:   0.2\n",
      "0.302043\n",
      "0.477599\n",
      "0.374196\n",
      "0.325572\n",
      "0.24509\n",
      "0.384502\n",
      "0.343386\n",
      "0.318892\n",
      "0.312622\n",
      "0.194711\n",
      "Average loss at step 520:   0.3\n",
      "0.279163\n",
      "0.212248\n",
      "0.362697\n",
      "0.335464\n",
      "0.221274\n",
      "0.271957\n",
      "0.355776\n",
      "0.297286\n",
      "0.215966\n",
      "0.307082\n",
      "Average loss at step 530:   0.3\n",
      "0.230019\n",
      "0.259815\n",
      "0.265632\n",
      "0.432196\n",
      "0.289373\n",
      "0.220274\n",
      "0.211712\n",
      "0.276653\n",
      "0.188595\n",
      "0.391224\n",
      "Average loss at step 540:   0.3\n",
      "0.12916\n",
      "0.208374\n",
      "0.235977\n",
      "0.222212\n",
      "0.465276\n",
      "0.195709\n",
      "0.175712\n",
      "0.28238\n",
      "0.25456\n",
      "0.362716\n",
      "Average loss at step 550:   0.3\n",
      "0.268213\n",
      "0.165969\n",
      "0.160667\n",
      "0.125785\n",
      "0.23163\n",
      "0.222055\n",
      "0.188436\n",
      "0.242872\n",
      "0.2931\n",
      "0.228737\n",
      "Average loss at step 560:   0.2\n",
      "0.154863\n",
      "0.202268\n",
      "0.197767\n",
      "0.339436\n",
      "0.279147\n",
      "0.229898\n",
      "0.232277\n",
      "0.363804\n",
      "0.286631\n",
      "0.33068\n",
      "Average loss at step 570:   0.3\n",
      "0.417632\n",
      "0.391875\n",
      "0.267514\n",
      "0.35906\n",
      "0.304623\n",
      "0.356898\n",
      "0.354442\n",
      "0.264876\n",
      "0.281939\n",
      "0.245425\n",
      "Average loss at step 580:   0.3\n",
      "0.322613\n",
      "0.32213\n",
      "0.22809\n",
      "0.30359\n",
      "0.312369\n",
      "0.375973\n",
      "0.274524\n",
      "0.20702\n",
      "0.242069\n",
      "0.171452\n",
      "Average loss at step 590:   0.3\n",
      "0.200121\n",
      "0.290147\n",
      "0.239495\n",
      "0.158803\n",
      "0.384388\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    # to visualize using TensorBoard\n",
    "    writer = tf.summary.FileWriter('./my_graph/mnist', sess.graph)\n",
    "    ##### You have to create folders to store checkpoints\n",
    "    ckpt = tf.train.get_checkpoint_state(os.path.dirname('./checkpoints/convnet_mnist/checkpoint'))\n",
    "    # if that checkpoint exists, restore from checkpoint\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    \n",
    "    initial_step = global_step.eval()\n",
    "\n",
    "    start_time = time.time()\n",
    "    n_batches = int(mnist.train.num_examples / BATCH_SIZE)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    for index in range(initial_step, n_batches * N_EPOCHS): # train the model n_epochs times\n",
    "        X_batch, Y_batch = mnist.train.next_batch(BATCH_SIZE)\n",
    "        _, loss_batch = sess.run([optimizer, loss], \n",
    "                                feed_dict={X: X_batch, Y:Y_batch, dropout: DROPOUT}) \n",
    "        total_loss += loss_batch\n",
    "        print(loss_batch)\n",
    "        if (index + 1) % SKIP_STEP == 0:\n",
    "            print(\"Average loss at step {}: {:5.1f}\".format(index + 1, total_loss / SKIP_STEP))\n",
    "            total_loss = 0.0\n",
    "            saver.save(sess, './checkpoints/convnet_mnist/mnist-convnet', index)\n",
    "    \n",
    "    print(\"Optimization Finished!\") # should be around 0.35 after 25 epochs\n",
    "    print(\"Total time: {0} seconds\".format(time.time() - start_time))\n",
    "    \n",
    "    # test the model\n",
    "    n_batches = int(mnist.test.num_examples/BATCH_SIZE)\n",
    "    total_correct_preds = 0\n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = mnist.test.next_batch(BATCH_SIZE)\n",
    "        _, loss_batch, logits_batch = sess.run([optimizer, loss, logits], \n",
    "                                        feed_dict={X: X_batch, Y:Y_batch, dropout: DROPOUT}) \n",
    "        preds = tf.nn.softmax(logits_batch)\n",
    "        correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))\n",
    "        accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "        total_correct_preds += sess.run(accuracy)   \n",
    "    \n",
    "    print(\"Accuracy {0}\".format(total_correct_preds/mnist.test.num_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

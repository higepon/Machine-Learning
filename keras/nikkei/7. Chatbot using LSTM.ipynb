{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is this?\n",
    "For stock prices prediction, we will probably text input or sentiment input.\n",
    "To get familiar with those kind of input, I think it's good to learn them by developing Chatbot.\n",
    "It's also fun :)\n",
    "\n",
    "### Steps\n",
    "- Input data: Use my own twitter data\n",
    "- Shape the input data\n",
    "- Build a model\n",
    "- Train\n",
    "- Test\n",
    "- Visualize\n",
    "\n",
    "### Input Data\n",
    "I'm going to use my own twitter data.\n",
    "\n",
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Install mecab\n",
    "    # Thanks to http://qiita.com/taroc/items/b9afd914432da08dafc8\n",
    "    brew install mecab\n",
    "    brew install mecab-ipadic\n",
    "    pip install mecab-python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " , BOS/EOS\n",
      "最近 , 名詞\n",
      "自分 , 名詞\n",
      "は , 助詞\n",
      "どこ , 名詞\n",
      "に , 助詞\n",
      "行こ , 動詞\n",
      "う , 助動詞\n",
      "と , 助詞\n",
      "し , 動詞\n",
      "てる , 動詞\n",
      "ん , 名詞\n",
      "だろ , 助動詞\n",
      "と , 助詞\n",
      "思い , 動詞\n",
      "ながら , 助詞\n",
      "トレーニング , 名詞\n",
      "し , 動詞\n",
      "てる , 動詞\n",
      "😉 , 記号\n",
      " , BOS/EOS\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "mt = MeCab.Tagger(\"mecabrc\")\n",
    "mt.parse('') # prevent string gc-ed huh?\n",
    "node = mt.parseToNode(\"最近自分はどこに行こうとしてるんだろと思いながらトレーニングしてる😉\")\n",
    "while node:\n",
    "    word = node.surface\n",
    "    pos = node.feature.split(\",\")[0]\n",
    "    print('{0} , {1}'.format(word, pos))\n",
    "    node = node.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BOS/EOS',\n",
       " '最近',\n",
       " '自分',\n",
       " 'は',\n",
       " 'どこ',\n",
       " 'に',\n",
       " '行こ',\n",
       " 'う',\n",
       " 'と',\n",
       " 'し',\n",
       " 'てる',\n",
       " 'ん',\n",
       " 'だろ',\n",
       " 'と',\n",
       " '思い',\n",
       " 'ながら',\n",
       " 'トレーニング',\n",
       " 'し',\n",
       " 'てる',\n",
       " '😉',\n",
       " 'BOS/EOS']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import MeCab\n",
    "mt = MeCab.Tagger(\"mecabrc\")\n",
    "mt.parse('') # prevent string gc-ed huh?\n",
    "\n",
    "def tokenize(text):\n",
    "    ret = []\n",
    "    node = mt.parseToNode(text)\n",
    "    while node:\n",
    "        word = node.surface\n",
    "        if word == '':\n",
    "            word = 'BOS/EOS'\n",
    "        ret.append(word)\n",
    "        node = node.next\n",
    "    return ret\n",
    "\n",
    "tokenize(\"最近自分はどこに行こうとしてるんだろと思いながらトレーニングしてる😉\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Seq2Seq\n",
    "See http://qiita.com/halhorn/items/dc10596942ef4be54af5\n",
    "\n",
    "    git clone https://www.github.com/datalogai/recurrentshop.git\n",
    "    cd recurrentshop\n",
    "    python setup.py install\n",
    "    pip3 install git+https://github.com/farizrahman4u/seq2seq.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from seq2seq.models import SimpleSeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = SimpleSeq2Seq(input_dim=1, hidden_dim=10, output_length=8, output_dim=1)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.models.Sequential object at 0x11b30ea58>\n",
      "cMhNVE7msy\n"
     ]
    }
   ],
   "source": [
    "import random as random\n",
    "import string as string\n",
    "\n",
    "def randomString(n):\n",
    "    return ''.join([random.choice(string.ascii_letters + string.digits) for i in range(n)])\n",
    "\n",
    "print(randomString(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['g7EAd', 'przaF', 'UjkGS', 'ymZxw', 'Lc2E9', 'XbzpE', 'Er1Rc', 'moe90', 'em9SN', '6T11D', 'd9DGY', 'KRlXR', '3Z6iK', 'cFJQ9', 'PhcRD', 'ubm02', 'bQWgD', 'k536R', 'isOkm', 'XC2lM', 'Ct8Gp', 'FNZKB', 'R4bJK', 'R2ljI', 'd2VAx', 'sUyDB', '5QJpn', 'FzLDC', 'XVIEC', 'luv6W', 'WEm8Q', 'qGcDl', '6dmqr', '5UZwT', 'Y4Y2V', 'fex2t', 'NVO6c', 'AzVVn', 'SZris', 'EVL6i', 'QAt5e', 'eURi5', 'WOmFd', 'EGi5t', 'cIMfe', 'vWmmn', 'c4UpQ', 'MQ3dX', '7vnmg', '58eR3']\n"
     ]
    }
   ],
   "source": [
    "## Create Input data\n",
    "num_input = 50\n",
    "print([randomString(5) for _ in range(num_input)])\n",
    "\n",
    "## never mind, this is not necesary for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps\n",
    "- Understand http://qiita.com/halhorn/items/dc10596942ef4be54af5\n",
    "- Make above work here\n",
    "- Change above a little bit\n",
    "- Use 10 pairs of replies and train then predict\n",
    "- Run above via real data\n",
    "- Can we use word2vec above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 8, 1)\n"
     ]
    }
   ],
   "source": [
    "## from http://qiita.com/halhorn/items/dc10596942ef4be54af5\n",
    "from seq2seq.models import SimpleSeq2Seq\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "# シンプルな Seq2Seq モデルを構築\n",
    "model = SimpleSeq2Seq(input_dim=1, hidden_dim=10, output_length=8, output_dim=1)\n",
    "\n",
    "# 学習の設定\n",
    "model.compile(loss='mse', optimizer='rmsprop')\n",
    "\n",
    "# データ作成\n",
    "# 入力：1000パターンの位相を持つ一次元のサイン波\n",
    "# 出力：各入力の逆位相のサイン波\n",
    "a = np.random.random(1000)\n",
    "x = np.array([np.sin([[p] for p in np.arange(0, 0.8, 0.1)] + aa) for aa in a])\n",
    "print(x.shape)\n",
    "y = -x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model target: expected recurrentcontainer_16 to have shape (None, 16, 1) but got array with shape (1000, 8, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-282a36153bce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# 学習\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 未学習のデータでテスト\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/higepon/.pyenv/versions/anaconda3-4.1.1/envs/tensorflow/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    670\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/Users/higepon/.pyenv/versions/anaconda3-4.1.1/envs/tensorflow/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\u001b[0m\n\u001b[1;32m   1114\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1117\u001b[0m         \u001b[0;31m# prepare validation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/higepon/.pyenv/versions/anaconda3-4.1.1/envs/tensorflow/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1031\u001b[0m                                    \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m                                    \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m                                    exception_prefix='model target')\n\u001b[0m\u001b[1;32m   1034\u001b[0m         sample_weights = standardize_sample_weights(sample_weight,\n\u001b[1;32m   1035\u001b[0m                                                     self.output_names)\n",
      "\u001b[0;32m/Users/higepon/.pyenv/versions/anaconda3-4.1.1/envs/tensorflow/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    122\u001b[0m                             \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                             \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                             str(array.shape))\n\u001b[0m\u001b[1;32m    125\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model target: expected recurrentcontainer_16 to have shape (None, 16, 1) but got array with shape (1000, 8, 1)"
     ]
    }
   ],
   "source": [
    "\n",
    "# 学習\n",
    "model.fit(x, y, nb_epoch=5, batch_size=32)\n",
    "\n",
    "# 未学習のデータでテスト\n",
    "x_test = np.array([np.sin([[p] for p in np.arange(0, 0.8, 0.1)] + aa) for aa in np.arange(0, 1.0, 0.1)])\n",
    "y_test = -x_test\n",
    "print(model.evaluate(x_test, y_test, batch_size=32))\n",
    "\n",
    "# 未学習のデータで生成\n",
    "predicted = model.predict(x_test, batch_size=32)\n",
    "\n",
    "plt.plot(np.arange(0, 0.8, 0.1), [xx[0] for xx in x_test[9]])\n",
    "plt.plot(np.arange(0, 0.8, 0.1), [xx[0] for xx in predicted[9]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conversations = [\n",
    "    (\"オンライン証券会社、ETFなどの商品の良さ、手数料の安さで考えると最近はどこがいいんだろう？２つほど口座持ってるんだけど5年前くらいに選んだものだからベストかわからず。 @karino2012 さんとかどうですか。\", \"最近の事は自分も分からんですねぇ。7年くらい前からE-Tradeと楽天の2つがあれば良さそうだったけど、、、\"),\n",
    "    (\"なぜ決定木を使う人が多いんだろう。\", \"識別能力が高くて安定しているからじゃないか。\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len_tweets 62\n",
      "max_len_replies 16\n",
      "[['BOS/EOS', 'オンライン', '証券', '会社', '、', 'ETF', 'など', 'の', '商品', 'の', '良', 'さ', '、', '手数料', 'の', '安', 'さ', 'で', '考える', 'と', '最近', 'は', 'どこ', 'が', 'いい', 'ん', 'だろ', 'う', '？', '２つ', 'ほど', '口座', '持っ', 'てる', 'ん', 'だ', 'けど', '5', '年', '前', 'くらい', 'に', '選ん', 'だ', 'もの', 'だ', 'から', 'ベスト', 'か', 'わから', 'ず', '。', '@', 'karino', '2012', 'さん', 'とか', 'どう', 'です', 'か', '。', 'BOS/EOS'], ['BOS/EOS', '最近', 'の', '事', 'は', '自分', 'も', '分から', 'ん', 'です', 'ねぇ', '。', '7', '年', 'くらい', '前', 'から', 'E', '-', 'Trade', 'と', '楽天', 'の', '2', 'つ', 'が', 'あれ', 'ば', '良', 'さ', 'そう', 'だっ', 'た', 'けど', '、', '、', '、', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS']]\n"
     ]
    }
   ],
   "source": [
    "tweets = conversations[0]\n",
    "replies = conversations[1]\n",
    "\n",
    "tweets_lemmatized = [tokenize(sentence) for sentence in tweets]\n",
    "max_len_tweets = len(max(tweets_lemmatized, key=len))\n",
    "print(\"max_len_tweets\", max_len_tweets)\n",
    "\n",
    "replies_lemmatized = [tokenize(sentence) for sentence in replies]\n",
    "max_len_replies = len(max(replies_lemmatized, key=len))\n",
    "print(\"max_len_replies\", max_len_replies)\n",
    "\n",
    "tweets_lemmatized = [tweet + ['BOS/EOS'] * (max_len_tweets - len(tweet)) for tweet in tweets_lemmatized]\n",
    "replies_lemmatized = [reply + ['BOS/EOS'] * (max_len_replies - len(reply)) for reply in replies_lemmatized]\n",
    "\n",
    "# pad BOS/EOS\n",
    "print(tweets_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words {'て', 'ベスト', '7', 'さん', 'BOS/EOS', '2', '木', 'など', 'た', 'くらい', 'の', 'も', 'に', '選ん', '決定', 'う', '良', 'で', '会社', 'そう', '、', 'ほど', 'と', '安', 'ねぇ', '考える', '証券', 'ば', 'てる', 'か', 'わから', '2012', 'Trade', '分から', 'つ', 'あれ', '口座', 'E', 'なぜ', 'どう', '識別', '5', 'ない', '安定', 'ず', 'じゃ', 'けど', '使う', 'し', '事', '２つ', 'だっ', 'オンライン', '自分', '。', '商品', 'が', '？', 'ETF', '年', '持っ', 'だ', 'いる', '-', 'どこ', '能力', '高く', '最近', '@', 'さ', '前', '手数料', 'もの', 'ん', 'から', '多い', 'とか', 'を', 'karino', 'は', 'いい', '人', 'だろ', 'です', '楽天'}\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "words = set(itertools.chain(*tweets_lemmatized)).union(set(itertools.chain(*replies_lemmatized)))\n",
    "print(\"words\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2idx {'て': 0, 'ベスト': 1, '多い': 75, 'ず': 44, '7': 2, 'さん': 3, 'BOS/EOS': 4, '木': 6, 'など': 7, 'し': 48, 'た': 8, '２つ': 50, 'karino': 78, 'くらい': 9, 'の': 10, '自分': 53, 'に': 12, '選ん': 13, 'だっ': 51, 'から': 74, 'もの': 72, 'じゃ': 45, 'オンライン': 52, '決定': 14, 'う': 15, '良': 16, '。': 54, 'だ': 61, 'で': 17, 'あれ': 35, '会社': 18, '2': 5, '@': 68, '？': 57, 'そう': 19, 'ETF': 58, '年': 59, 'けど': 46, '、': 20, '持っ': 60, 'ねぇ': 24, 'と': 22, '安': 23, '人': 81, '高く': 66, 'ほど': 21, '-': 63, 'どこ': 64, '証券': 26, 'ば': 27, 'てる': 28, 'E': 37, 'か': 29, 'わから': 30, '使う': 47, '2012': 31, 'だろ': 82, 'さ': 69, '事': 49, '手数料': 71, 'ん': 73, '商品': 55, '前': 70, '能力': 65, 'です': 83, 'Trade': 32, '分から': 33, 'つ': 34, '口座': 36, 'も': 11, 'は': 79, 'なぜ': 38, 'を': 77, 'いい': 80, 'どう': 39, '最近': 67, '識別': 40, 'いる': 62, '5': 41, 'が': 56, '考える': 25, 'ない': 42, '楽天': 84, 'とか': 76, '安定': 43}\n",
      "idx2word ['て', 'ベスト', '7', 'さん', 'BOS/EOS', '2', '木', 'など', 'た', 'くらい', 'の', 'も', 'に', '選ん', '決定', 'う', '良', 'で', '会社', 'そう', '、', 'ほど', 'と', '安', 'ねぇ', '考える', '証券', 'ば', 'てる', 'か', 'わから', '2012', 'Trade', '分から', 'つ', 'あれ', '口座', 'E', 'なぜ', 'どう', '識別', '5', 'ない', '安定', 'ず', 'じゃ', 'けど', '使う', 'し', '事', '２つ', 'だっ', 'オンライン', '自分', '。', '商品', 'が', '？', 'ETF', '年', '持っ', 'だ', 'いる', '-', 'どこ', '能力', '高く', '最近', '@', 'さ', '前', '手数料', 'もの', 'ん', 'から', '多い', 'とか', 'を', 'karino', 'は', 'いい', '人', 'だろ', 'です', '楽天']\n"
     ]
    }
   ],
   "source": [
    "# dictionaries for converting words to integers and vice versa\n",
    "word2idx = dict((v, i) for i, v in enumerate(words))\n",
    "print(\"word2idx\", word2idx)\n",
    "\n",
    "idx2word = list(words)\n",
    "print(\"idx2word\", idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets_idx [[4, 52, 26, 18, 20, 58, 7, 10, 55, 10, 16, 69, 20, 71, 10, 23, 69, 17, 25, 22, 67, 79, 64, 56, 80, 73, 82, 15, 57, 50, 21, 36, 60, 28, 73, 61, 46, 41, 59, 70, 9, 12, 13, 61, 72, 61, 74, 1, 29, 30, 44, 54, 68, 78, 31, 3, 76, 39, 83, 29, 54, 4], [4, 67, 10, 49, 79, 53, 11, 33, 73, 83, 24, 54, 2, 59, 9, 70, 74, 37, 63, 32, 22, 84, 10, 5, 34, 56, 35, 27, 16, 69, 19, 51, 8, 46, 20, 20, 20, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]]\n",
      "replies_idx [[4, 38, 14, 6, 77, 47, 81, 56, 75, 73, 82, 15, 54, 4, 4, 4], [4, 40, 65, 56, 66, 0, 43, 48, 0, 62, 74, 45, 42, 29, 54, 4]]\n"
     ]
    }
   ],
   "source": [
    "# convert the sentences a numpy array\n",
    "to_idx = lambda x: [word2idx[word] for word in x]\n",
    "\n",
    "tweets_idx = [to_idx(tweet) for tweet in tweets_lemmatized]\n",
    "replies_idx = [to_idx(tweet) for tweet in replies_lemmatized]\n",
    "\n",
    "print(\"tweets_idx\", tweets_idx)\n",
    "print(\"replies_idx\", replies_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape (2, 62, 1)\n",
      "Y.shape (2, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(tweets_idx).reshape(-1, max_len_tweets, 1)\n",
    "#print(\"X\", X)\n",
    "print(\"X.shape\", X.shape)\n",
    "\n",
    "Y = np.asarray(replies_idx).reshape(-1, max_len_replies, 1)\n",
    "print(\"Y.shape\", Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2/2 [==============================] - 3s - loss: 2332.6660\n",
      "Epoch 2/5\n",
      "2/2 [==============================] - 0s - loss: 2331.1233\n",
      "Epoch 3/5\n",
      "2/2 [==============================] - 0s - loss: 2329.5732\n",
      "Epoch 4/5\n",
      "2/2 [==============================] - 0s - loss: 2328.0342\n",
      "Epoch 5/5\n",
      "2/2 [==============================] - 0s - loss: 2326.5234\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12dbe1f60>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SimpleSeq2Seq(input_dim=1, hidden_dim=10, output_length=max_len_replies, output_dim=1, depth=3)\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "model.fit(X, Y, nb_epoch=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BOS/EOS', 'いい', '人', 'だろ', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS', 'BOS/EOS']\n",
      "[4, 80, 81, 82, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0.00495657],\n",
       "        [ 0.01447026],\n",
       "        [ 0.02855   ],\n",
       "        [ 0.046618  ],\n",
       "        [ 0.06772517],\n",
       "        [ 0.09072167],\n",
       "        [ 0.11441334],\n",
       "        [ 0.13770361],\n",
       "        [ 0.1597044 ],\n",
       "        [ 0.17979693],\n",
       "        [ 0.1976383 ],\n",
       "        [ 0.21312347],\n",
       "        [ 0.22632433],\n",
       "        [ 0.23742503],\n",
       "        [ 0.24666676],\n",
       "        [ 0.25430775]]], dtype=float32)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_str=\"いい人だろ\"\n",
    "lemmatized = tokenize(test_str)\n",
    "lemmatized = lemmatized + ['BOS/EOS'] * (max_len_tweets - len(lemmatized))\n",
    "print(lemmatized)\n",
    "\n",
    "idx = to_idx(lemmatized)\n",
    "print(idx)\n",
    "model.predict(np.array(idx).reshape(1, max_len_tweets, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
